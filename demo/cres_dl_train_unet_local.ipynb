{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qAJ7tvnVtGo"
   },
   "source": [
    "# CRES Deep Learning: \n",
    "## --- *Image Segmentation with UNET* --- \n",
    "\n",
    "-----\n",
    "**Overview**\n",
    "\n",
    "* Below is an example of how to load a simulated dataset and train an image segmentation model on it.\n",
    "\n",
    "-----\n",
    "**Instructions**\n",
    "\n",
    "* Start by uploading a .zip file with the .spec file dataset. This may take a second. \n",
    "    0. Start by following the directions on the [README](https://github.com/Helium6CRES/he6-cres-deep-learning) to make a training dataset. Compress the root directory that contains both the spec files and labels.\n",
    "    1. Click on files on right menu bar. \n",
    "    2. Click upload. \n",
    "    3. Upload a .zip containing data files. \n",
    "    4. Need to have instructions on the readme for how to create these files. \n",
    "* Then follow the cells below to visualize the dataset and train the lightning module. \n",
    "\n",
    "-----\n",
    "**Tips**\n",
    "\n",
    "* If you restart the runtime you don't lose all your imported data but if you restart and delete then you do. \n",
    "* If you change the runtime type (GPU to CPU for example) you lose all imported data. \n",
    "\n",
    "-----\n",
    "**Resources**\n",
    "\n",
    "* [Pytorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "* [Torchvision docs](https://pytorch.org/vision/stable/index.html)\n",
    "* [Useful for uploading to colab](https://medium.com/@vishakha1203/easiest-way-to-upload-large-datasets-to-google-colab-1f89231844dc)\n",
    "* [Discussion of how to optimize num workers in Dataloader](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813)\n",
    "\n",
    "-----\n",
    "**Project Links**: \n",
    "* [he6-cres-deep-learning github page](https://github.com/Helium6CRES/he6-cres-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq8zlPfK3-Vn"
   },
   "source": [
    "## 1.&nbsp;Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CPViS3XTH0pF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "## Put the below into a requirements.txt\n",
    "\n",
    "%%capture\n",
    "! pip install torch == 1.12.1\n",
    "! pip install torchvision == 0.13.1\n",
    "! pip install pytorch-lightning == 1.6.4\n",
    "! pip install pytorch-lightning-bolts\n",
    "! pip install torchmetrics == 0.9.1\n",
    "! pip install matplotlib == 3.1.3\n",
    "! pip install numpy == 1.21.6\n",
    "! pip install ipywidgets==7.7.0\n",
    "! pip install re==2.2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n0H8m0yZHu0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deep learning imports.\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks, make_grid\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# Standard imports. \n",
    "from typing import List, Union\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Necessary for creating our images.\n",
    "from skimage.draw import line_aa\n",
    "\n",
    "# Interactive widgets for data viz.\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tlsUIQL4LWV"
   },
   "source": [
    "**import deep learning package from Helium6CRES organization github page**\n",
    "\n",
    "* May need to git clone if you have not already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIe20y_iaowo",
    "outputId": "61b0abf3-bef9-418b-f215-ab4d806936b0"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r8kL8ND9vetT",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'he6_cres_deep_learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nm/071v5pbd70ngwv53cdyygz8m0000gn/T/ipykernel_780/2125807828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Documents/dsir-1031/project/he6-cres-deep-learning/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhe6_cres_deep_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_learning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhe6_cres_deep_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_learning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhe6_cres_deep_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeep_learning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'he6_cres_deep_learning'"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "sys.path.append('~/Documents/dsir-1031/project/he6-cres-deep-learning/')\n",
    "from he6_cres_deep_learning.deep_learning import ds\n",
    "from he6_cres_deep_learning.deep_learning import util \n",
    "from he6_cres_deep_learning.deep_learning import model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOuBRcAGcHo3"
   },
   "source": [
    "## 3.&nbsp;Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JnNN1fw5scD0"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/media/drew/T7 Shield/cres_deep_learning/training_data/config/simple_ds\"\n",
    "cres_dm = ds.CRES_DM(root_dir = dataset_path, max_pool = 16, file_max = 12, batch_size= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "b4_q1vEPqFZ0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31118af8c47c4f8799679c649c8c4c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='display_num_imgs', max=8, style=SliderStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "virtual void QEventDispatcherUNIX::registerSocketNotifier(QSocketNotifier*): Multiple socket notifiers for same socket 11 and type Read\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "@interact\n",
    "def vizualize_label_targets(display_num_imgs= widgets.IntSlider(style= style,value=4,min=0,max=8,step=1, description = \"display_num_imgs\"),\n",
    "                            mask_alpha= widgets.FloatSlider(style= style,value=.4,min=0,max=1,step=.01, description = \"mask alpha\"),\n",
    "                            display_size = widgets.IntSlider(style= style, value=15,min=5,max=50,step=1),\n",
    "                            show_labels = widgets.Checkbox(style= style,value=True,description='target masks'),   \n",
    "                            ): \n",
    "\n",
    "\n",
    "    dataiter = iter(cres_dm.train_dataloader())\n",
    "\n",
    "    imgs, labels = dataiter.next()\n",
    "\n",
    "    imgs = 255 - imgs.repeat(1, 3, 1, 1)\n",
    "    imgs = imgs[:display_num_imgs]\n",
    "    labels = labels[:display_num_imgs]\n",
    "    masks = util.labels_to_masks(labels)\n",
    "\n",
    "    result_images = [imgs[i] for i in range(display_num_imgs)]\n",
    "    \n",
    "    class_map={\n",
    "            0: {\n",
    "                \"name\": \"background\",\n",
    "                \"target_color\": (255, 255, 255),\n",
    "            },\n",
    "            1: {\"name\": \"band 0\", \"target_color\": (255, 0, 0)},\n",
    "            2: {\"name\": \"band 1\", \"target_color\": (0, 255, 0)},\n",
    "            3: {\"name\": \"band 2\", \"target_color\": (0, 0, 255)},\n",
    "        }\n",
    "\n",
    "    if show_labels: \n",
    "        result_images = util.display_masks_unet(imgs, masks, cres_dm.class_map, alpha = mask_alpha)\n",
    "\n",
    "    grid = make_grid(result_images)\n",
    "    util.show(grid, figsize = (display_size, display_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xFlNqGehn5k"
   },
   "source": [
    "## 4.&nbsp;Train the Lightning Module\n",
    "\n",
    "* Too many spec/label files can overpower the ram. Start with `file_max` = 10, `max_pool` = 16 below. \n",
    "* If you have more classes you will need to change the `weights` tensor to have more values and the `num_classes` to match `max(labels)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "S_xx4gR08rnZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params | In sizes         | Out sizes       \n",
      "-------------------------------------------------------------------------------------\n",
      "0 | train_acc | Accuracy         | 0      | ?                | ?               \n",
      "1 | train_f1  | F1Score          | 0      | ?                | ?               \n",
      "2 | train_iou | JaccardIndex     | 0      | ?                | ?               \n",
      "3 | val_acc   | Accuracy         | 0      | ?                | ?               \n",
      "4 | val_f1    | F1Score          | 0      | ?                | ?               \n",
      "5 | loss      | CrossEntropyLoss | 0      | ?                | ?               \n",
      "6 | model     | UNET             | 30.5 K | [4, 1, 256, 256] | [4, 2, 256, 256]\n",
      "-------------------------------------------------------------------------------------\n",
      "30.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.5 K    Total params\n",
      "0.122     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce3fe3ca16948ba9683309dfa67e468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"/media/drew/T7 Shield/cres_deep_learning/training_data/config/simple_ds\"\n",
    "cres_dm = ds.CRES_DM(root_dir = dataset_path, max_pool = 16, file_max = 10, batch_size = 4)\n",
    "\n",
    "# Define weights for loss function. \n",
    "weights = torch.tensor([1,10]).float()\n",
    "\n",
    "# Create callback for ModelCheckpoints. \n",
    "checkpoint_callback = ModelCheckpoint(filename='{epoch:02d}', save_top_k = 50, monitor = \"Loss/val_loss\", every_n_epochs = 1)\n",
    "\n",
    "# Define Logger. \n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"cres_image_segmentation\", log_graph = True)\n",
    "\n",
    "# Create Instance of Lightning Module. \n",
    "img_seg_lm = model.LightningImageSegmentation(in_channels=1, \n",
    "                                        num_classes=2, \n",
    "                                        first_feature_num = 4, \n",
    "                                        num_layers = 2, \n",
    "                                        skip_connect = True, \n",
    "                                        kernel_size = 3, \n",
    "                                        bias = False, \n",
    "                                        weight_loss = weights)\n",
    "\n",
    "# -----------Set device.------------------\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create an instance of a Trainer.\n",
    "trainer = pl.Trainer(logger = logger, callbacks = [checkpoint_callback], accelerator = device, max_epochs = 15, log_every_n_steps = 2)\n",
    "\n",
    "# Fit. \n",
    "trainer.fit(img_seg_lm, cres_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8YgICfqpp5G"
   },
   "source": [
    "**tensorboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "M2AqWA-CjuDW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6444), started 0:46:30 ago. (Use '!kill 6444' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e719bae100e967df\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e719bae100e967df\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frtldUbWicMI"
   },
   "source": [
    "## 5.&nbsp; Visualize Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3R1KOfXicML"
   },
   "source": [
    "**get test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jH3Cb6pPicMM"
   },
   "outputs": [],
   "source": [
    "test_dataiter = iter(cres_dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJv63TDicMN"
   },
   "source": [
    "**execute following cell again to see more test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "-gjIvoYJicMP"
   },
   "outputs": [],
   "source": [
    "imgs, labels = test_dataiter.next()\n",
    "masks = util.labels_to_masks(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTmPOYXVicMQ"
   },
   "source": [
    "**visualize predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fwOIli8KicMS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d73595a40a43a79ff06b8851c4bfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='epoch', max=4), Checkbox(value=False, description='displâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "version = 4\n",
    "\n",
    "@interact\n",
    "def vizualize_labels_preds( epoch = widgets.IntSlider(value=9,min=0,max=4,step=1),\n",
    "                            show_labels = widgets.Checkbox(value=False,description='display_labels'),\n",
    "                            show_preds = widgets.Checkbox(value=False,description='display_preds'),\n",
    "                            display_size = widgets.IntSlider(value=10,min=2,max=50,step=1), \n",
    "                            mask_threshold = widgets.FloatSlider(value=.5,min=0,max=1,step=.0001,description='mask_thresh'),\n",
    "                        ): \n",
    "\n",
    "  \n",
    "    PATH = '/home/drew/He6CRES/he6-cres-deep-learning/demo/tb_logs/cres_image_segmentation/version_{}\\\n",
    "    /checkpoints/epoch={:02d}.ckpt'.format(version, epoch)\n",
    "  \n",
    "    loaded_lm = model.LightningImageSegmentation.load_from_checkpoint(PATH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_lm(imgs)\n",
    "\n",
    "    probs = logits.softmax(dim = 1)\n",
    "\n",
    "    imgs_display = 255 - imgs.repeat(1, 3, 1, 1)\n",
    "    result_image = [imgs_display[i] for i in range(len(imgs_display))]\n",
    "\n",
    "    if  show_labels: \n",
    "        result_image = util.display_masks_unet(result_image, masks, cres_dm.class_map)\n",
    "    if  show_preds: \n",
    "        preds = (probs > mask_threshold)\n",
    "        result_image = util.display_masks_unet(result_image, preds, cres_dm.class_map)\n",
    "    grid = make_grid(result_image)\n",
    "    util.show(grid, figsize = (display_size,display_size), extent = [0, 35, 0, 1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tGWof3D4e9k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5xBV8mSKdFBg",
    "d_mCF1VKdEu1",
    "Hufce6HUR15P",
    "9DIfGCUHrsuT",
    "dlVcFEdM_Lrj"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
