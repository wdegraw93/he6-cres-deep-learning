{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6aeaee-6b85-4194-beaa-2e21663a0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, fixed\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json\n",
    "from typing import List, Union\n",
    "import gc\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Deep learning imports.\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes, make_grid\n",
    "from torchvision.ops import masks_to_boxes, box_area\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision, compute_area\n",
    "\n",
    "# Additional settings.\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "\n",
    "sys.path.append(sys.path[0]+'/..')\n",
    "from he6_cres_deep_learning.daq import DAQ, Config\n",
    "root_dir = sys.path[0]+'/config/fasterRCNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926cea4-973f-45ca-a986-ae1b8530665c",
   "metadata": {},
   "source": [
    "#### Define Dataset class that will load spec files and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b96239-b392-4a6d-a405-1673bdca5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRES_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"DOCUMENT.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, root_dir, freq_bins=4096, max_pool=3, file_max=10, transform=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the spec files and targets.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.freq_bins = freq_bins\n",
    "        self.max_pool = max_pool\n",
    "        self.file_max = file_max\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs, self.targets = self.collect_imgs_and_targets()\n",
    "\n",
    "        # Guarentee the correct type.\n",
    "        self.imgs = self.imgs.type(torch.ByteTensor)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = self.imgs[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def collect_imgs_and_targets(self):\n",
    "\n",
    "        img_dir = self.root_dir + \"/spec_files\"\n",
    "        target_dir = self.root_dir + \"/label_files\"\n",
    "\n",
    "        # TODO: make it so directories get spec_prefix instead of files\n",
    "        imgs, exp_name = self.load_spec_dir(img_dir)\n",
    "#---------------------------------------------------------------------------------------------------        \n",
    "        # Is this really the best way to scale the bboxes?\n",
    "        targets = self.load_target_dir(target_dir, exp_name, imgs[0][0].shape)\n",
    "        # targets = targets.long()\n",
    "\n",
    "        return imgs, targets\n",
    "\n",
    "    def load_spec_dir(self, dir_path):\n",
    "        \"\"\"\n",
    "        Loads all of the images in a directory into torch images.\n",
    "\n",
    "        Args:\n",
    "            dir_path (str): path should point to a directory that only contains\n",
    "                .JPG images. Or any image type compatible with cv2.imread().\n",
    "\n",
    "            resize_factor (float): how to resize the image. Often one would\n",
    "                like to reduce the size of the images to be easier/faster to\n",
    "                use with our maskrcnn model.\n",
    "\n",
    "        Returns:\n",
    "            imgs (List[torch.ByteTensor[3, H, W]]): list of images (each a\n",
    "                torch.ByteTensor of shape(3, H, W)).\n",
    "        \"\"\"\n",
    "        path_glob = Path(dir_path).glob(\"**/*\")\n",
    "        files = [x for x in path_glob if x.is_file()]\n",
    "        file_names = [str(x.name) for x in files]\n",
    "        files = [str(x) for x in files]\n",
    "        \n",
    "        # extract experiment name to match to target file \n",
    "        exp_name = list(set(re.findall(r'[a-zA-Z0-9]+', name)[0] for name in file_names))\n",
    "        \n",
    "        # Extract the file index from the file name.\n",
    "        file_idxs = [int(re.findall(r\"\\d+\", name)[0]) for name in file_names]\n",
    "\n",
    "        # Sort the files list based on the file_idx.\n",
    "        files = [\n",
    "            file\n",
    "            for (file, file_idx) in sorted(\n",
    "                zip(files, file_idxs), key=lambda pair: pair[1]\n",
    "            )\n",
    "        ]\n",
    "        # Maxpool to use on images and labels.\n",
    "        maxpool = nn.MaxPool2d(self.max_pool, self.max_pool, return_indices=False)\n",
    "\n",
    "        if len(files) == 0:\n",
    "            raise UserWarning(\"No files found at the input path.\")\n",
    "\n",
    "        imgs = []\n",
    "        for file in files[: self.file_max]:\n",
    "            img = self.spec_to_numpy(file)\n",
    "            img = torch.from_numpy(img).unsqueeze(0)\n",
    "            img = img.permute(0, 2, 1)\n",
    "\n",
    "            # Apply max pooling now so we never have to hold the large images.\n",
    "\n",
    "            imgs.append(maxpool(img.float()))\n",
    "\n",
    "        imgs = torch.stack(imgs)\n",
    "\n",
    "        return imgs, exp_name\n",
    "    \n",
    "    def load_target_dir(self, dir_path, exp_name, spec_shape): # spec_shape[0] is frequency, spec_shape[1] is time\n",
    "        \"\"\"\n",
    "        TODO: Document\n",
    "        Load bbox json files\n",
    "        \"\"\"\n",
    "        path_glob = Path(dir_path).glob(f\"{exp_name}*\")\n",
    "        files = [x for x in path_glob if x.is_file()]\n",
    "        files = [str(x) for x in files]\n",
    "\n",
    "        if len(files) == 0:\n",
    "            raise UserWarning(\"No files found at the input path.\")\n",
    "        \n",
    "        # targets will be a list of dicts that contain the bboxes, labels, and scores\n",
    "        targets = []\n",
    "        targets_dict = {'boxes': [],\n",
    "                        'labels': []}\n",
    "        for file in files[: self.file_max]:\n",
    "            # read all bboxes for experiment\n",
    "            with open(file, 'r') as f:\n",
    "                bboxes = json.load(f)\n",
    "                \n",
    "                # each value corresponds to a file number\n",
    "                for file_num, bbox_dict in bboxes.items():\n",
    "                    # make sure we've populated the dict at least once before appending to list\n",
    "                    if file_num != '0':\n",
    "                        targets.append(targets_dict)\n",
    "                        targets_dict = {'boxes': [],\n",
    "                                        'labels': []}\n",
    "                        \n",
    "                    # each bbox corresponds to an event in the file\n",
    "                    for bbox in bbox_dict.values():\n",
    "                        # apply maxpooling reduction before appending\n",
    "                        bbox = torch.tensor(bbox)/self.max_pool\n",
    "                        bbox = bbox.round().int()\n",
    "                        # max pooling can lead to tracks with no pixel width, avoid this\n",
    "                        if bbox[3] == bbox[1]:\n",
    "                            bbox[3] += 1\n",
    "                        if bbox[2] == bbox[0]:\n",
    "                            bbox[2] += 1\n",
    "                        targets_dict['boxes'].append(bbox)\n",
    "                        targets_dict['labels'].append(torch.tensor([1]))\n",
    "                        \n",
    "        targets.append(targets_dict)\n",
    "        targets_dict = {'boxes': [],\n",
    "                        'labels': []}\n",
    "        for target in targets:\n",
    "            target['boxes'] = torch.stack(target['boxes'])\n",
    "            target['labels'] = torch.tensor(target['labels'])\n",
    "        return targets\n",
    "        \n",
    "\n",
    "    def spec_to_numpy(\n",
    "        self, spec_path, slices=-1, packets_per_slice=1, start_packet=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        TODO: Document.\n",
    "        Making this just work for one packet per spectrum because that works for simulation in Katydid.\n",
    "        * Make another function that works with 4 packets per spectrum (for reading the Kr data).\n",
    "        \"\"\"\n",
    "\n",
    "        BYTES_IN_PAYLOAD = self.freq_bins\n",
    "        BYTES_IN_HEADER = 32\n",
    "        BYTES_IN_PACKET = BYTES_IN_PAYLOAD + BYTES_IN_HEADER\n",
    "\n",
    "        if slices == -1:\n",
    "            spec_array = np.fromfile(spec_path, dtype=\"uint8\", count=-1).reshape(\n",
    "                (-1, BYTES_IN_PACKET)\n",
    "            )[:, BYTES_IN_HEADER:]\n",
    "        else:\n",
    "            spec_array = np.fromfile(\n",
    "                spec_path, dtype=\"uint8\", count=BYTES_IN_PAYLOAD * slices\n",
    "            ).reshape((-1, BYTES_IN_PACKET))[:, BYTES_IN_HEADER:]\n",
    "        \n",
    "        if packets_per_slice > 1:\n",
    "\n",
    "            spec_flat_list = [\n",
    "                spec_array[(start_packet + i) % packets_per_slice :: packets_per_slice]\n",
    "                for i in range(packets_per_slice)\n",
    "            ]\n",
    "            spec_flat = np.concatenate(spec_flat_list, axis=1)\n",
    "            spec_array = spec_flat\n",
    "\n",
    "        return spec_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111570ad-7a7d-4f8f-9f7a-33beac0b7c54",
   "metadata": {},
   "source": [
    "#### Define DataModule that will handle train/val/test splitting of data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5dd1350-0014-45d3-a8cf-5db641402086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRES_DM(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    Self contained PyTorch Lightning DataModule for testing image\n",
    "    segmentation models with PyTorch Lightning. Uses the torch dataset\n",
    "    ImageSegmentation_DS.\n",
    "\n",
    "    Args:\n",
    "        train_val_size (int): total size of the training and validation\n",
    "            sets combined.\n",
    "        train_val_split (Tuple[float, float]): should sum to 1.0. For example\n",
    "            if train_val_size = 100 and train_val_split = (0.80, 0.20)\n",
    "            then the training set will contain 80 imgs and the validation\n",
    "            set will contain 20 imgs.\n",
    "        test_size (int): the size of the test data set.\n",
    "        batch_size (int): batch size to be input to dataloaders. Applies\n",
    "            for training, val, and test datasets.\n",
    "\n",
    "    Notes: For now you can decide to shuffle the entire dataset or not but\n",
    "    the train is always shuffled and the val/test isn't so you can look at\n",
    "    the same images easily.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        freq_bins=4096,\n",
    "        max_pool=8,\n",
    "        file_max=10,\n",
    "        transform=None,\n",
    "        train_val_test_splits=(0.6, 0.3, 0.1),\n",
    "        batch_size=1,\n",
    "        shuffle_dataset=True,\n",
    "        seed=42,\n",
    "        num_workers=0,\n",
    "        class_map={\n",
    "            0: {\n",
    "                \"name\": \"background\",\n",
    "                \"target_color\": (255, 255, 255),\n",
    "            },\n",
    "            1: {\"name\": \"event\", \"target_color\": (255, 0, 0)}\n",
    "        },\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Attributes.\n",
    "        self.root_dir = root_dir\n",
    "        self.freq_bins = freq_bins\n",
    "        self.max_pool = max_pool\n",
    "        self.file_max = file_max\n",
    "        self.transform = transform\n",
    "        self.class_map = class_map\n",
    "        self.train_val_test_splits = train_val_test_splits\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle_dataset = shuffle_dataset\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        self.cres_dataset = CRES_Dataset(\n",
    "            self.root_dir,\n",
    "            freq_bins=self.freq_bins,\n",
    "            max_pool=self.max_pool,\n",
    "            file_max=self.file_max,\n",
    "            transform=self.transform,\n",
    "        )\n",
    "\n",
    "        # Creating data indices for training and validation splits:\n",
    "        dataset_size = len(self.cres_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        splits = self.train_val_test_splits\n",
    "        split_idxs = [\n",
    "            int(np.floor(splits[0] * dataset_size)),\n",
    "            int(np.floor((splits[0] + splits[1]) * dataset_size)),\n",
    "        ]\n",
    "\n",
    "        if self.shuffle_dataset:\n",
    "            rng = np.random.default_rng(self.seed)\n",
    "            rng.shuffle(indices)\n",
    "\n",
    "        train_indices, val_indices, test_indices = (\n",
    "            indices[: split_idxs[0]],\n",
    "            indices[split_idxs[0] : split_idxs[1]],\n",
    "            indices[split_idxs[1] :],\n",
    "        )\n",
    "\n",
    "        # Creating PT data samplers and loaders. For now only train is shuffled.\n",
    "        self.train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        self.val_sampler = torch.utils.data.SequentialSampler(val_indices)\n",
    "        self.test_sampler = torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        When dealing with lists of target dictionaries one needs to be \n",
    "        careful how the batches are collated. The default pytorch dataloader \n",
    "        behaviour is to return a single dictionary for the whole batch of \n",
    "        images which won't work as input to the mask rcnn model. Instead \n",
    "        we want a list of dictionaries; one for each image. See here for \n",
    "        more details on the dataloader collate_fn:\n",
    "        https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3\n",
    "\n",
    "        Returns:\n",
    "            imgs (torch.UInt8Tensor[batch_size, 3, img_size, img_size]): \n",
    "                batch of images.\n",
    "            targets (List[Dict[torch.Tensor]]): list of dictionaries of \n",
    "                length batch_size.\n",
    "\n",
    "        \"\"\"\n",
    "        imgs = []\n",
    "        targets = []\n",
    "\n",
    "        for img, target in batch:\n",
    "            imgs.append(img)\n",
    "            targets.append(target)\n",
    "\n",
    "        # Converts list of tensor images (of shape (3,H,W) and len batch_size)\n",
    "        # into a tensor of shape (batch_size, 3, H, W).\n",
    "        imgs = torch.stack(imgs)\n",
    "\n",
    "        return imgs, targets\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cres_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.train_sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cres_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.val_sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cres_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=self.test_sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a54633-3f1f-4602-bf08-658f04ad9b52",
   "metadata": {},
   "source": [
    "#### Function to load the model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07867e46-13fa-4162-9d3c-49949358992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fasterrcnn(num_classes=2, pretrained=True):\n",
    "    \"\"\"A function for loading the PyTorch implementation of FasterRCNN.\n",
    "    To not have predictor changed at all set num_classes = -1.\n",
    "    See here for documentation on the input and output specifics:\n",
    "    https://pytorch.org/vision/stable/models/faster_rcnn.html\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of output classes desired.\n",
    "        pretrained (bool): whether or not to load a model pretrained on the COCO dataset. \n",
    "    \"\"\"\n",
    "\n",
    "    # load Faster RCNN pre-trained model\n",
    "    if pretrained:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    else:\n",
    "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "\n",
    "    if num_classes != -1:\n",
    "        # get the number of input features\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a89f8-9ea8-4995-8cf4-fdf17e44a3d3",
   "metadata": {},
   "source": [
    "#### Function for 'overfitting' of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02860e54-1134-47cf-8249-0b58621cd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit(imgs, targets, model, optimizer,  device=None,  epochs= 100): \n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Formatting for input to model. \n",
    "    imgs_normed = imgs / 255.0\n",
    "    imgs_normed = imgs_normed.to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        loss_dict = model(imgs_normed, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch %25 == 0:\n",
    "            print(f\"epoch: {epoch}\")\n",
    "            print(f\"loss {losses:.4f}\\n\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "163109ec-8a31-4153-af80-3634f091def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cres_dm = CRES_DM(root_dir, max_pool=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d29ce0f-f556-4969-9919-70b5dcbf9a93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(cres_dm\u001b[38;5;241m.\u001b[39mtrain_dataloader())\n\u001b[1;32m      4\u001b[0m imgs, targets \u001b[38;5;241m=\u001b[39m dataiter\u001b[38;5;241m.\u001b[39mnext()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFaster RCNN target for img 0:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m targets\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "cres_dm.setup(stage = \"fit\")\n",
    "dataiter = iter(cres_dm.train_dataloader())\n",
    "\n",
    "imgs, targets = dataiter.next()\n",
    "\n",
    "print(f\"Input shape:\\n {imgs.shape} \\n\" )\n",
    "\n",
    "print(f\"Faster RCNN target for img 0:\\n \")\n",
    "for key, val in targets.items():\n",
    "    print(f\"{key}:\\n {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2acd93b-5836-4b8c-bbc6-cc98ee78f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = get_fasterrcnn(num_classes = 2, pretrained = True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "overfit(imgs, targets, model, optimizer, device, epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6da0a0c7-a2b2-4e89-b829-c605b5aceef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_score_cut(preds, score_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Takes a list of prediction dictionaries (one for each image) and cuts\n",
    "    out all instances whose score is below the score threshold.\n",
    "\n",
    "    Args:\n",
    "        preds (List[Dict[torch.Tensor]]): predictions as output by the\n",
    "            torchvision implimentation of MaskRCNN or FasterRCNN. The \n",
    "            scores are in the range (0,1) and signify the certainty of \n",
    "            the model for that instance.\n",
    "            See link below for details on the target/prediction formatting.\n",
    "            https://pytorch.org/vision/0.12/_modules/torchvision/models/detection/mask_rcnn.html\n",
    "        score_threshold (float): the threshold to apply to the identified\n",
    "            objects. If an instance is below the score_threshold it will\n",
    "            be removed from the score_thresholded_preds dictionary.\n",
    "\n",
    "    Returns:\n",
    "        score_thresholded_preds (List[Dict[torch.Tensor]]): predictions\n",
    "            that exceed score_threshold.\n",
    "    \"\"\"\n",
    "    score_thresholded_preds = [\n",
    "        {key: value[pred[\"scores\"] > score_threshold] for key, value in pred.items()}\n",
    "        for pred in preds\n",
    "    ]\n",
    "\n",
    "    return score_thresholded_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a04d1-c672-4c3c-8b47-a21ceb8ecf37",
   "metadata": {},
   "source": [
    "#### Define LightningModule class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c3049c4-8001-474e-8dd0-fa248aaf2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRES_LM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes = 2, lr = 3e-4, pretrained = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # LM Attributes.\n",
    "        self.num_classes = num_classes\n",
    "        self.pretrained = pretrained\n",
    "        self.lr = lr\n",
    "\n",
    "        # Log hyperparameters. \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics.\n",
    "        # self.iou = JaccardIndex(task='binary')\n",
    "        # self.map_bbox = MeanAveragePrecision(iou_type = \"bbox\", class_metrics = False)\n",
    "\n",
    "        # Faster RCNN model. \n",
    "        self.model = self.get_fasterrcnn_model(self.num_classes, self.pretrained)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        self.model.eval()\n",
    "        imgs_normed = self.norm_imgs(imgs)\n",
    "        return self.model(imgs_normed)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "\n",
    "        imgs, targets = train_batch\n",
    "        imgs_normed = self.norm_imgs(imgs)\n",
    "\n",
    "        loss_dict = self.model(imgs_normed, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        self.log('Loss/train_loss', losses)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "\n",
    "        imgs, targets = val_batch\n",
    "        preds = self.forward(imgs)\n",
    "        \n",
    "        \n",
    "        iou_list = torch.tensor([box_iou(target[\"boxes\"], pred[\"boxes\"]).diag().mean() for target, pred in zip(targets, preds)])\n",
    "        # print(iou_list)\n",
    "        self.log('IoU_bbox/val',iou_list)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def configure_optimizers(self): \n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def get_fasterrcnn_model(self, num_classes, pretrained):\n",
    "        \n",
    "        # load Faster RCNN pre-trained model\n",
    "        if pretrained: \n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "        else: \n",
    "            model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "        \n",
    "        # get the number of input features \n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "\n",
    "        return model\n",
    "\n",
    "    def norm_imgs(self, imgs): \n",
    "\n",
    "        imgs_normed = imgs.float() / 255.0\n",
    "\n",
    "        return imgs_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c9e4d52-7990-4ae4-9dd5-e1b4b113d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training object\n",
    "cres_dm = CRES_DM(root_dir,\n",
    "                  max_pool=16,\n",
    "                  file_max=1000,\n",
    "                  batch_size=4,\n",
    "                  num_workers=4\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8fa0ea-b063-4ab6-99d8-e2e3089bd996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1, 256, 320]),\n",
       " 1000,\n",
       " {'boxes': tensor([[205, 144, 320, 146],\n",
       "          [ 59, 250, 250, 252],\n",
       "          [298, 189, 320, 190],\n",
       "          [148, 160, 320, 167],\n",
       "          [172, 118, 320, 125]], dtype=torch.int32),\n",
       "  'labels': tensor([1, 1, 1, 1, 1])})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cres_dm.cres_dataset.imgs.shape, len(cres_dm.cres_dataset.targets), cres_dm.cres_dataset.targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253588a-b29b-483f-8ca0-0d052f25416c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | FasterRCNN | 41.3 M\n",
      "-------------------------------------\n",
      "41.1 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.3 M    Total params\n",
      "165.197   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016726970672607422,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0111, 0.0000, 0.0007, 0.0140])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0084, 0.0082])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017920494079589844,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac844c5921444cca473319929b6acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01735210418701172,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1868, 0.2417, 0.0950])\n",
      "tensor([0.0000, 0.3538, 0.1598, 0.1001])\n",
      "tensor([0.0000, 0.0339, 0.4004, 0.0000])\n",
      "tensor([0.2709, 0.1168, 0.0000, 0.0639])\n",
      "tensor([0.1087, 0.2300, 0.1185, 0.1430])\n",
      "tensor([0.0000, 0.1497, 0.1386, 0.1015])\n",
      "tensor([0.0000, 0.0000, 0.2625, 0.2109])\n",
      "tensor([0.1285, 0.4133, 0.4767, 0.1631])\n",
      "tensor([0.1152, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.7095, 0.0000, 0.1134, 0.1976])\n",
      "tensor([0.1406, 0.4031, 0.0468, 0.0000])\n",
      "tensor([0.1323, 0.0804, 0.1452, 0.0000])\n",
      "tensor([0.5456, 0.3481, 0.0000, 0.2603])\n",
      "tensor([0.1286, 0.2058, 0.1250, 0.0604])\n",
      "tensor([0.5825, 0.0000, 0.4316, 0.6580])\n",
      "tensor([0.7738, 0.1676, 0.2706, 0.1312])\n",
      "tensor([0.2306, 0.0000, 0.1304, 0.1865])\n",
      "tensor([0.2084, 0.1691, 0.0489, 0.2223])\n",
      "tensor([0.1993, 0.0000, 0.3161, 0.0000])\n",
      "tensor([0.1448, 0.6521, 0.3355, 0.2061])\n",
      "tensor([0.1619, 0.1392, 0.4915, 0.1042])\n",
      "tensor([0.0854, 0.1453, 0.0540, 0.0000])\n",
      "tensor([0.0000, 0.0939, 0.2523, 0.0883])\n",
      "tensor([0.0000, 0.3907, 0.0000, 0.2455])\n",
      "tensor([0.0000, 0.1658, 0.0992, 0.2206])\n",
      "tensor([0.1112, 0.0000, 0.0000, 0.1589])\n",
      "tensor([0.1707, 0.0499, 0.0000, 0.6027])\n",
      "tensor([0.2192, 0.2300, 0.5445, 0.4822])\n",
      "tensor([0.1114, 0.0000, 0.0000, 0.2424])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.2876])\n",
      "tensor([0.0331, 0.5719, 0.7275, 0.1030])\n",
      "tensor([0.0000, 0.4056, 0.0938, 0.0000])\n",
      "tensor([0.0000, 0.1365, 0.1599, 0.0000])\n",
      "tensor([0.0000, 0.1027, 0.0000, 0.1849])\n",
      "tensor([0.0000, 0.0000, 0.1124, 0.1203])\n",
      "tensor([0.2375, 0.1175, 0.0053, 0.0786])\n",
      "tensor([0.1319, 0.3581, 0.1446, 0.0000])\n",
      "tensor([0.7379, 0.0000, 0.0000, 0.0828])\n",
      "tensor([0.0977, 0.1474, 0.1178, 0.0945])\n",
      "tensor([0.1971, 0.0831, 0.2702, 0.3862])\n",
      "tensor([0.6317, 0.0554, 0.3994, 0.2267])\n",
      "tensor([0.0392, 0.4988, 0.0000, 0.1695])\n",
      "tensor([0.0000, 0.1914, 0.1672, 0.0320])\n",
      "tensor([0.1743, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.1133, 0.2150, 0.0000, 0.0961])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.1386])\n",
      "tensor([0.0000, 0.2454, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.4975, 0.2252, 0.0723])\n",
      "tensor([0.2197, 0.0000, 0.3925, 0.1780])\n",
      "tensor([0.2228, 0.1293, 0.0000, 0.0000])\n",
      "tensor([0.3086, 0.2454, 0.2893, 0.1109])\n",
      "tensor([0.0000, 0.2894, 0.1672, 0.1227])\n",
      "tensor([0.1459, 0.0000, 0.5978, 0.0355])\n",
      "tensor([0.1876, 0.0000, 0.1707, 0.2927])\n",
      "tensor([0.1293, 0.2318, 0.3496, 0.1536])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.2097])\n",
      "tensor([0.1377, 0.0304, 0.0000, 0.6343])\n",
      "tensor([0.2804, 0.1760, 0.1724, 0.1133])\n",
      "tensor([0.1933, 0.0614, 0.0000, 0.0487])\n",
      "tensor([0.1426, 0.2165, 0.0000, 0.0399])\n",
      "tensor([0.1776, 0.1359, 0.0000, 0.0000])\n",
      "tensor([0.0914, 0.0000, 0.2211, 0.0000])\n",
      "tensor([0.0508, 0.0000, 0.6225, 0.1240])\n",
      "tensor([0.0000, 0.1760, 0.0000, 0.0000])\n",
      "tensor([0.2106, 0.1035, 0.0075, 0.1275])\n",
      "tensor([0.1638, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0454, 0.2266, 0.3734, 0.2455])\n",
      "tensor([0.1257, 0.0000, 0.1592, 0.1844])\n",
      "tensor([0.0589, 0.5759, 0.2669, 0.2636])\n",
      "tensor([0.1516, 0.0982, 0.0000, 0.1698])\n",
      "tensor([0.2383, 0.5689, 0.4461, 0.0987])\n",
      "tensor([0.2723, 0.1016, 0.0000, 0.0049])\n",
      "tensor([0.1455, 0.0000, 0.0000, 0.1354])\n",
      "tensor([0.3689, 0.1827, 0.0839, 0.0000])\n",
      "tensor([0.0000, 0.4630, 0.0000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017683982849121094,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.0000, 0.2493, 0.0000, 0.1468])\n",
      "tensor([0.0000, 0.0718, 0.0000, 0.0533])\n",
      "tensor([0.1058, 0.0000, 0.0837, 0.1630])\n",
      "tensor([0.1829, 0.1629, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.2090, 0.1406])\n",
      "tensor([0.2692, 0.0000, 0.0000, 0.1244])\n",
      "tensor([0.0000, 0.2645, 0.0000, 0.2215])\n",
      "tensor([0.0000, 0.2548, 0.3141, 0.0000])\n",
      "tensor([0.8185, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.2187, 0.4513, 0.4588, 0.0000])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.2645, 0.2500, 0.0000, 0.2464])\n",
      "tensor([0.3348, 0.1133, 0.0218, 0.2547])\n",
      "tensor([0.4342, 0.1534, 0.0000, 0.7147])\n",
      "tensor([0.9468, 0.1142, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.1698, 0.1159, 0.0000])\n",
      "tensor([0.1700, 0.0000, 0.1549, 0.1521])\n",
      "tensor([0.0000, 0.1915, 0.2750, 0.1527])\n",
      "tensor([0.2259, 0.3225, 0.2278, 0.0000])\n",
      "tensor([0.1713, 0.1202, 0.6566, 0.1042])\n",
      "tensor([0.3058, 0.3473, 0.0000, 0.1367])\n",
      "tensor([0.0000, 0.3020, 0.2292, 0.2156])\n",
      "tensor([0.0000, 0.4603, 0.0000, 0.0158])\n",
      "tensor([0.1953, 0.0000, 0.2356, 0.3043])\n",
      "tensor([0.0710, 0.1492, 0.0000, 0.0045])\n",
      "tensor([0.0000, 0.0459, 0.0000, 0.5939])\n",
      "tensor([0.1860, 0.0000, 0.5318, 0.0000])\n",
      "tensor([0.2412, 0.0000, 0.0811, 0.4239])\n",
      "tensor([0.5466, 0.1790, 0.0000, 0.3968])\n",
      "tensor([0.0000, 0.3877, 0.5994, 0.0949])\n",
      "tensor([0.0000, 0.7164, 0.1038, 0.0000])\n",
      "tensor([0.0643, 0.1562, 0.0000, 0.1144])\n",
      "tensor([0.0450, 0.1075, 0.0000, 0.2660])\n",
      "tensor([0.2481, 0.0000, 0.0000, 0.2556])\n",
      "tensor([0.1679, 0.0000, 0.1667, 0.0000])\n",
      "tensor([0.0884, 0.4768, 0.0000, 0.0289])\n",
      "tensor([0.6836, 0.0000, 0.4503, 0.1127])\n",
      "tensor([0.1024, 0.3158, 0.0000, 0.1189])\n",
      "tensor([0.5799, 0.1655, 0.1534, 0.0594])\n",
      "tensor([0.2807, 0.0000, 0.2995, 0.3071])\n",
      "tensor([0.0276, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.4052, 0.2949, 0.0466])\n",
      "tensor([0.0889, 0.1970, 0.0000, 0.5249])\n",
      "tensor([0.0928, 0.0000, 0.0000, 0.2588])\n",
      "tensor([0.0000, 0.0843, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.1365, 0.0000])\n",
      "tensor([0.1438, 0.3126, 0.0000, 0.0978])\n",
      "tensor([0.0000, 0.1825, 0.4730, 0.2725])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.2543, 0.0000, 0.0000, 0.1607])\n",
      "tensor([0.0000, 0.7492, 0.1237, 0.0000])\n",
      "tensor([0.2368, 0.0000, 0.5313, 0.0488])\n",
      "tensor([0.2798, 0.0942, 0.0000, 0.1796])\n",
      "tensor([0.2028, 0.5091, 0.1628, 0.2225])\n",
      "tensor([0.2775, 0.0000, 0.4706, 0.2736])\n",
      "tensor([0.0068, 0.0000, 0.0393, 0.5930])\n",
      "tensor([0.0000, 0.1741, 0.3603, 0.2022])\n",
      "tensor([0.1140, 0.0000, 0.0000, 0.0537])\n",
      "tensor([0.0537, 0.1105, 0.2514, 0.3226])\n",
      "tensor([0.2207, 0.1611, 0.0000, 0.0866])\n",
      "tensor([0.1779, 0.1313, 0.8287, 0.0000])\n",
      "tensor([0.1194, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.2351, 0.1237])\n",
      "tensor([0.3110, 0.0000, 0.0000, 0.1794])\n",
      "tensor([0.0000, 0.0000, 0.1326, 0.1131])\n",
      "tensor([0.1144, 0.0000, 0.6067, 0.7165])\n",
      "tensor([0.0000, 0.7168, 0.0000, 0.2786])\n",
      "tensor([0.1172, 0.4501, 0.6291, 0.0000])\n",
      "tensor([0.1863, 0.1822, 0.0000, 0.0000])\n",
      "tensor([0.6176, 0.8094, 0.1575, 0.0644])\n",
      "tensor([0.0000, 0.0000, 0.0986, 0.0378])\n",
      "tensor([0.1066, 0.0572, 0.1978, 0.2064])\n",
      "tensor([0.3254, 0.1091, 0.1706, 0.0822])\n",
      "tensor([0.1906, 0.0000, 0.0667])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020226716995239258,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.1806, 0.0000, 0.1323])\n",
      "tensor([0.0000, 0.2888, 0.2181, 0.1030])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.1258, 0.0912, 0.0000, 0.3011])\n",
      "tensor([0.0000, 0.1818, 0.0000, 0.0000])\n",
      "tensor([0.0685, 0.0000, 0.3122, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.1327, 0.1042])\n",
      "tensor([0.1752, 0.2731, 0.0000, 0.2401])\n",
      "tensor([0.0000, 0.0000, 0.2599, 0.2243])\n",
      "tensor([0.7536, 0.2531, 0.1135, 0.0000])\n",
      "tensor([0.2229, 0.4969, 0.0927, 0.0953])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.1245])\n",
      "tensor([0.3087, 0.3527, 0.0000, 0.2366])\n",
      "tensor([0.0000, 0.0000, 0.1087, 0.1699])\n",
      "tensor([0.4361, 0.1697, 0.2079, 0.7946])\n",
      "tensor([0.9018, 0.2591, 0.0000, 0.0113])\n",
      "tensor([0.0000, 0.0058, 0.1640, 0.0000])\n",
      "tensor([0.1597, 0.2287, 0.1349, 0.1589])\n",
      "tensor([0.1840, 0.0000, 0.1750, 0.2676])\n",
      "tensor([0.3157, 0.0000, 0.4286, 0.0000])\n",
      "tensor([0.2621, 0.0000, 0.6190, 0.2867])\n",
      "tensor([0.0862, 0.1997, 0.0000, 0.1141])\n",
      "tensor([0.0000, 0.0000, 0.2854, 0.1170])\n",
      "tensor([0.0200, 0.5267, 0.1965, 0.0100])\n",
      "tensor([0.1408, 0.1950, 0.4524, 0.3138])\n",
      "tensor([0.0000, 0.2814, 0.0000, 0.0000])\n",
      "tensor([0.0362, 0.0000, 0.0000, 0.6534])\n",
      "tensor([0.0000, 0.0000, 0.6426, 0.2680])\n",
      "tensor([0.0733, 0.1489, 0.0812, 0.2043])\n",
      "tensor([0.3615, 0.1638, 0.0000, 0.5625])\n",
      "tensor([0.0000, 0.4543, 0.8495, 0.1347])\n",
      "tensor([0.2866, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.8649, 0.0778, 0.6845])\n",
      "tensor([0.1493, 0.0000, 0.0000, 0.4270])\n",
      "tensor([0.2929, 0.0000, 0.0000, 0.2645])\n",
      "tensor([0.1868, 0.0000, 0.1296, 0.0000])\n",
      "tensor([0.1249, 0.5520, 0.0000, 0.3258])\n",
      "tensor([0.6895, 0.1124, 0.0861, 0.0704])\n",
      "tensor([0.0337, 0.0000, 0.0000, 0.4504])\n",
      "tensor([0.6868, 0.2204, 0.4369, 0.0723])\n",
      "tensor([0.5300, 0.0484, 0.2618, 0.2850])\n",
      "tensor([0.0000, 0.7424, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.3447, 0.6100, 0.1099])\n",
      "tensor([0.0480, 0.2710, 0.0000, 0.3009])\n",
      "tensor([0.0000, 0.1456, 0.0000, 0.0000])\n",
      "tensor([0.0267, 0.0000, 0.0000, 0.1732])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.1528, 0.5259, 0.0000, 0.0392])\n",
      "tensor([0.0000, 0.2332, 0.4686, 0.7425])\n",
      "tensor([0.1073, 0.0000, 0.0000, 0.1030])\n",
      "tensor([0.1891, 0.5724, 0.0000, 0.2796])\n",
      "tensor([0.0000, 0.4439, 0.0000, 0.0000])\n",
      "tensor([0.2294, 0.0000, 0.7879, 0.0308])\n",
      "tensor([0.0000, 0.1258, 0.3699, 0.1386])\n",
      "tensor([0.1747, 0.7656, 0.1918, 0.2194])\n",
      "tensor([0.0000, 0.0000, 0.4881, 0.6057])\n",
      "tensor([0.1943, 0.0719, 0.1286, 0.7654])\n",
      "tensor([0.0000, 0.1651, 0.1260, 0.1140])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.3240, 0.2083, 0.2716, 0.0000])\n",
      "tensor([0.1053, 0.4044, 0.0000, 0.0000])\n",
      "tensor([0.1507, 0.1303, 0.8427, 0.3078])\n",
      "tensor([0.0123, 0.0000, 0.6286, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.2440, 0.1449])\n",
      "tensor([0.1868, 0.0296, 0.0000, 0.1663])\n",
      "tensor([0.0000, 0.0000, 0.4186, 0.0000])\n",
      "tensor([0.0469, 0.5524, 0.4779, 0.7689])\n",
      "tensor([0.0000, 0.3529, 0.3236, 0.2473])\n",
      "tensor([0.2628, 0.4281, 0.3583, 0.2937])\n",
      "tensor([0.1124, 0.1885, 0.2175, 0.0000])\n",
      "tensor([0.6162, 0.9086, 0.0534, 0.1763])\n",
      "tensor([0.2122, 0.1508, 0.0000, 0.1522])\n",
      "tensor([0.1995, 0.0946, 0.1524, 0.0000])\n",
      "tensor([0.5101, 0.1154, 0.1615, 0.0000])\n",
      "tensor([0.1935, 0.0000, 0.1070])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02582263946533203,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.3163, 0.1744, 0.2661])\n",
      "tensor([0.0000, 0.2915, 0.0544, 0.0876])\n",
      "tensor([0.0000, 0.0287, 0.0000, 0.0811])\n",
      "tensor([0.1381, 0.2881, 0.0000, 0.3403])\n",
      "tensor([0.3080, 0.1473, 0.0457, 0.0000])\n",
      "tensor([0.1426, 0.0000, 0.0000, 0.2068])\n",
      "tensor([0.0000, 0.0000, 0.4114, 0.0000])\n",
      "tensor([0.1334, 0.2983, 0.0000, 0.0932])\n",
      "tensor([0.0000, 0.0000, 0.0741, 0.0000])\n",
      "tensor([0.8880, 0.3714, 0.0000, 0.0000])\n",
      "tensor([0.2299, 0.3486, 0.0962, 0.0000])\n",
      "tensor([0.0000, 0.1179, 0.0000, 0.0000])\n",
      "tensor([0.2913, 0.4541, 0.0000, 0.2394])\n",
      "tensor([0.3323, 0.5938, 0.0168, 0.1211])\n",
      "tensor([0.5861, 0.1771, 0.7816, 0.8452])\n",
      "tensor([0.9375, 0.3355, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0411, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0825, 0.2710])\n",
      "tensor([0.0000, 0.1671, 0.0561, 0.2628])\n",
      "tensor([0.1905, 0.0000, 0.4335, 0.0000])\n",
      "tensor([0.3094, 0.0000, 0.6984, 0.1745])\n",
      "tensor([0.1539, 0.1248, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.2314, 0.1280])\n",
      "tensor([0.0000, 0.6190, 0.2342, 0.0000])\n",
      "tensor([0.0947, 0.2115, 0.2807, 0.2625])\n",
      "tensor([0.0303, 0.2750, 0.0661, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.1879, 0.8626])\n",
      "tensor([0.0438, 0.2626, 0.7264, 0.0000])\n",
      "tensor([0.1227, 0.0000, 0.0891, 0.2448])\n",
      "tensor([0.5830, 0.1243, 0.0129, 0.4025])\n",
      "tensor([0.0000, 0.5872, 0.7924, 0.0560])\n",
      "tensor([0.0000, 0.0940, 0.0000, 0.0000])\n",
      "tensor([0.2229, 0.0000, 0.0000, 0.1942])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.3055])\n",
      "tensor([0.3059, 0.0000, 0.0000, 0.2384])\n",
      "tensor([0.2817, 0.0000, 0.0997, 0.0000])\n",
      "tensor([0.0447, 0.3822, 0.0000, 0.3646])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.1313])\n",
      "tensor([0.1284, 0.3228, 0.0000, 0.0000])\n",
      "tensor([0.6479, 0.0000, 0.2200, 0.1558])\n",
      "tensor([0.5545, 0.3004, 0.2828, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0165, 0.2270])\n",
      "tensor([0.0000, 0.4364, 0.4718, 0.0232])\n",
      "tensor([0.1778, 0.1760, 0.0000, 0.0653])\n",
      "tensor([0.0961, 0.1217, 0.0000, 0.1913])\n",
      "tensor([0.6788, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0679, 0.0000])\n",
      "tensor([0.1571, 0.8199, 0.2356, 0.1686])\n",
      "tensor([0.0000, 0.2107, 0.5298, 0.3304])\n",
      "tensor([0.2054, 0.0000, 0.0696, 0.0973])\n",
      "tensor([0.2199, 0.0000, 0.0000, 0.0442])\n",
      "tensor([0.0000, 0.4745, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.9113, 0.0743])\n",
      "tensor([0.0000, 0.3905, 0.4192, 0.1387])\n",
      "tensor([0.2160, 0.6354, 0.2312, 0.7661])\n",
      "tensor([0.0419, 0.1422, 0.0000, 0.2734])\n",
      "tensor([0.2936, 0.0000, 0.0000, 0.8427])\n",
      "tensor([0.0000, 0.2295, 0.3109, 0.1173])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([1.6939e-04, 1.8046e-01, 2.7365e-01, 2.0524e-02])\n",
      "tensor([0.1528, 0.2053, 0.0000, 0.4475])\n",
      "tensor([0.1660, 0.1391, 0.2753, 0.2320])\n",
      "tensor([0.1497, 0.2631, 0.4688, 0.0000])\n",
      "tensor([0.1908, 0.0000, 0.1530, 0.0256])\n",
      "tensor([0.0000, 0.0406, 0.0000, 0.4211])\n",
      "tensor([0.1217, 0.0842, 0.0054, 0.0000])\n",
      "tensor([0.0495, 0.4470, 0.5024, 0.7489])\n",
      "tensor([0.0631, 0.8551, 0.1971, 0.1951])\n",
      "tensor([0.2582, 0.6605, 0.4257, 0.1588])\n",
      "tensor([0.1518, 0.0791, 0.2782, 0.2608])\n",
      "tensor([0.0000, 0.8597, 0.0000, 0.4625])\n",
      "tensor([0.2168, 0.1178, 0.2272, 0.2868])\n",
      "tensor([0.0000, 0.1500, 0.2165, 0.0000])\n",
      "tensor([0.4365, 0.3286, 0.0000, 0.0000])\n",
      "tensor([0.2028, 0.0000, 0.1818])\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02080368995666504,
       "initial": 0,
       "n": 0,
       "ncols": 168,
       "nrows": 29,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.1056, 0.0042])\n",
      "tensor([0.0000, 0.2981, 0.0822, 0.1744])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.1574])\n",
      "tensor([0.1408, 0.1631, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.1159, 0.0000, 0.0000])\n",
      "tensor([0.0706, 0.0000, 0.1603, 0.2108])\n",
      "tensor([0.0910, 0.0000, 0.1619, 0.2215])\n",
      "tensor([0.1616, 0.3038, 0.0000, 0.2527])\n",
      "tensor([0.0000, 0.2582, 0.3598, 0.0000])\n",
      "tensor([0.9059, 0.1708, 0.1431, 0.0000])\n",
      "tensor([0.2229, 0.4336, 0.1240, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.2288])\n",
      "tensor([0.3259, 0.4528, 0.0000, 0.0000])\n",
      "tensor([0.3184, 0.0092, 0.0000, 0.1054])\n",
      "tensor([0.6764, 0.1862, 0.2048, 0.8982])\n",
      "tensor([0.8870, 0.3085, 0.0000, 0.0000])\n",
      "tensor([0.1201, 0.0103, 0.2136, 0.0000])\n",
      "tensor([0.2871, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.1629, 0.1699, 0.2363, 0.3630])\n",
      "tensor([0.0000, 0.0000, 0.4758, 0.1059])\n",
      "tensor([0.1219, 0.1628, 0.6869, 0.3686])\n",
      "tensor([0.1657, 0.1863, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0904, 0.2798, 0.0423])\n",
      "tensor([0.0000, 0.5160, 0.2867, 0.0000])\n",
      "tensor([0.1052, 0.0572, 0.2718, 0.2682])\n",
      "tensor([0.0276, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.1265, 0.0000, 0.1929, 0.7828])\n",
      "tensor([0.1170, 0.0000, 0.7301, 0.2465])\n",
      "tensor([0.1697, 0.0000, 0.0000, 0.4714])\n",
      "tensor([0.3641, 0.1729, 0.0617, 0.4392])\n",
      "tensor([0.0000, 0.3978, 0.6665, 0.1194])\n",
      "tensor([0.0000, 0.0007, 0.2504, 0.0000])\n",
      "tensor([0.1210, 0.3606, 0.1130, 0.1369])\n",
      "tensor([0.0314, 0.0000, 0.0000, 0.2111])\n",
      "tensor([0.3096, 0.0000, 0.1241, 0.1534])\n",
      "tensor([0.4111, 0.0000, 0.2026, 0.0769])\n",
      "tensor([0.0714, 0.4046, 0.0000, 0.0000])\n",
      "tensor([0.8230, 0.0000, 0.0000, 0.1348])\n",
      "tensor([0.1686, 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.7569, 0.1833, 0.2239, 0.1591])\n",
      "tensor([0.8109, 0.4007, 0.2295, 0.3190])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.1922])\n",
      "tensor([0.0000, 0.5204, 0.6837, 0.0205])\n",
      "tensor([0.0797, 0.2964, 0.0000, 0.2556])\n",
      "tensor([0.0599, 0.2454, 0.2627, 0.1092])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0.0000, 0.0000, 0.2925, 0.0000])\n",
      "tensor([0.1850, 0.5853, 0.0000, 0.0856])\n",
      "tensor([0.0000, 0.0000, 0.5131, 0.7375])\n",
      "tensor([0.1984, 0.0000, 0.0000, 0.0852])\n",
      "tensor([0.1981, 0.6098, 0.0000, 0.0317])\n",
      "tensor([0.3062, 0.4636, 0.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.7861, 0.0008])\n",
      "tensor([0.0000, 0.3844, 0.2311, 0.1315])\n",
      "tensor([0.2212, 0.7430, 0.3457, 0.8008])\n",
      "tensor([0.0773, 0.0000, 0.0000, 0.2100])\n",
      "tensor([0.2758, 0.0000, 0.0000, 0.7805])\n",
      "tensor([0.0000, 0.2440, 0.4211, 0.2251])\n",
      "tensor([0.0000, 0.1228, 0.0000, 0.0727])\n",
      "tensor([0.0523, 0.0000, 0.0089, 0.0259])\n",
      "tensor([0.0000, 0.3404, 0.0000, 0.1894])\n",
      "tensor([0.3174, 0.2503, 0.2728, 0.6569])\n",
      "tensor([0.0000, 0.1815, 0.7239, 0.0000])\n",
      "tensor([0.1374, 0.1349, 0.1897, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.2233])\n",
      "tensor([0.0000, 0.0722, 0.0000, 0.0444])\n",
      "tensor([0.0000, 0.6811, 0.4644, 0.7705])\n",
      "tensor([0.0000, 0.2860, 0.0000, 0.1843])\n",
      "tensor([0.2112, 0.7380, 0.4288, 0.1009])\n",
      "tensor([0.1213, 0.0476, 0.0245, 0.2878])\n",
      "tensor([0.7714, 0.8834, 0.1604, 0.0000])\n",
      "tensor([0.2857, 0.2487, 0.0000, 0.0031])\n",
      "tensor([0.5300, 0.0000, 0.1093, 0.2500])\n",
      "tensor([0.3883, 0.0000, 0.0801, 0.0000])\n",
      "tensor([0.1833, 0.0000, 0.0108])\n"
     ]
    }
   ],
   "source": [
    "# Create Instance of LightningModule\n",
    "cres_lm = CRES_LM(num_classes = 2, lr = 1e-4, pretrained = True)\n",
    "\n",
    "# Create callback for ModelCheckpoints. \n",
    "checkpoint_callback = ModelCheckpoint(filename='{epoch:02d}', \n",
    "                                      save_top_k = 15, \n",
    "                                      monitor = \"Loss/train_loss\", \n",
    "                                      every_n_epochs = 1)\n",
    "\n",
    "# Define Logger. \n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"cres\", log_graph = False)\n",
    "\n",
    "# Set device.\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create an instance of a Trainer.\n",
    "trainer = pl.Trainer(logger = logger, \n",
    "                     callbacks = [checkpoint_callback], \n",
    "                     accelerator = device, \n",
    "                     max_epochs = 10, \n",
    "                     log_every_n_steps = 1, \n",
    "                     check_val_every_n_epoch= 1)\n",
    "\n",
    "# Fit. \n",
    "trainer.fit(cres_lm, cres_dm.train_dataloader(), cres_dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ce2287-3f3e-4554-9075-de0cb6eeb683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2712), started 0:00:37 ago. (Use '!kill 2712' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eb1d0a96891f1b54\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eb1d0a96891f1b54\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd237774-4939-4f76-8fd3-8d1d9902947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'tb_logs/cres/version_{}/checkpoints/epoch={:02d}.ckpt'.format(52, 2)\n",
    "cres_lm = CRES_LM.load_from_checkpoint(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef1c70db-6f28-48f6-8b3a-091cd7a8ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cres_dm.setup(stage = \"test\")\n",
    "test_dataiter = iter(cres_dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16d64741-1085-45a5-9fd0-3b6260d847f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = test_dataiter.next()\n",
    "preds = cres_lm(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bafc19d-333c-400a-a131-5268a7609f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs, figsize=(10.0, 10.0)):\n",
    "\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), figsize=figsize, squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = TF.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "    return None\n",
    "\n",
    "def display_boxes(imgs, target_pred_dict, class_map, width = 1, fill=False):\n",
    "\n",
    "    num_imgs = len(imgs)\n",
    "    result_imgs = [\n",
    "        draw_bounding_boxes(\n",
    "            imgs[i].type(torch.uint8),\n",
    "            target_pred_dict[i][\"boxes\"].int(),\n",
    "            fill=fill,\n",
    "            colors=[\n",
    "                class_map[j.item()][\"target_color\"]\n",
    "                for j in target_pred_dict[i][\"labels\"]\n",
    "            ],\n",
    "            width=width,\n",
    "        )\n",
    "        for i in range(num_imgs)\n",
    "    ]\n",
    "\n",
    "    return result_imgs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b8bec57-dadf-4ab8-af30-3bacd3089b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5d9c2cb77045719c00fd13050ec5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='target boxes'), Checkbox(value=False, description='pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "@interact\n",
    "def vizualize_targets_predictions(\n",
    "                                target_box = widgets.Checkbox(value=False,description='target boxes'),\n",
    "                                pred_box = widgets.Checkbox(value=False,description='prediction boxes'),\n",
    "                                num_imgs= widgets.IntSlider(value=len(preds),min=0,max=len(preds),step=1, description = \"num_imgs\"),\n",
    "                                score_thresh = widgets.FloatSlider(value=.5,min=0,max=1,step=.0001, description = \"score_thresh\"),\n",
    "                                width =  widgets.IntSlider(value=1,min=1,max=10,step=1), \n",
    "                                display_size = widgets.IntSlider(value=20,min=2,max=50,step=1)\n",
    "                                ): \n",
    "\n",
    "    preds_cut = apply_score_cut(preds, score_threshold=score_thresh)\n",
    "    result_image = [imgs[i] for i in range(num_imgs)]\n",
    "\n",
    "    if target_box: \n",
    "        result_image = display_boxes(result_image, targets, cres_dm.class_map, fill = True)\n",
    "\n",
    "    if pred_box: \n",
    "        result_image = display_boxes(result_image, preds_cut, cres_dm.class_map)\n",
    "\n",
    "    grid = make_grid(result_image)\n",
    "    show(grid, figsize = (display_size, display_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd86624-e5de-4188-a757-24868d2e9784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
